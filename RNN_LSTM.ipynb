{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **RNN**\n",
    "A Recurrent Neural Network (RNN) is a type of neural network designed for sequential data (e.g., time series, text, audio). Unlike traditional neural networks (which treat each input independently), RNNs are designed to handle dependencies between elements in a sequence by maintaining a hidden state.\n",
    "\n",
    "### **Key Idea**\n",
    "* RNNs have a loop that allows information to persist over time.\n",
    "* The output at time step t depends not only on the input at time t but also on the output from the previous time step t-1.\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "At each time step t, and RNN updates its hidden state h_t using:\n",
    "$$ h_t = tanh(W_{hh}h_t + W_{xh}x_t + b_h) $$\n",
    "Where: <br>\n",
    "* $h_t$ = hidden state at time step t\n",
    "* $x_t$ = input at time step t\n",
    "* $W_{hh}, W_{xh}$ = weight matrices\n",
    "* $b_h$ = bias term\n",
    "* $tanh$ = activation function (usually tanh or ReLU)\n",
    "\n",
    "The output is computed as:\n",
    "$$ y_t = W_{hy}h_t + b_y $$\n",
    "\n",
    "Where: <br>\n",
    "* $y_t$ = output at time step t\n",
    "* $W_{hy}$ = weight matrix for output\n",
    "* $b_y$ = output bias\n",
    "\n",
    "## How Information is Passed Through Time:\n",
    "1. An initial hidden state $h_0$ is created (often initialized to zero)\n",
    "2. The input at each time step is combined with the hidden state from the previous step.\n",
    "3. The new hidden state is passed to the next time step.\n",
    "4. The output is generated based on the current hidden state.\n",
    "\n",
    "### **Strength of RNNS**\n",
    "* Good for processing sequences of variable length\n",
    "* Captures short-term dependencies\n",
    "* Works well for simple sequential tasks\n",
    "\n",
    "### **Problems with RNNs**\n",
    "1. Vanishing Gradient Problem:\n",
    "    * Gradient shrink too much during backpropagation, making it hard to update weights.\n",
    "2. Exploding Gradient Problem:\n",
    "    * Gradients grow too large and cause instability in training.\n",
    "3. Short-Term Memory Issue:\n",
    "    * RNNs struggle to remember long-term dependencies.\n",
    "\n",
    "\n",
    "# **LSTM**\n",
    "LSTM (Long Short-Term Memory) networks were introduced to solve the vanishing gradient problem and improve the ability to model long-term dependencies.\n",
    "\n",
    "### **Key Idea**\n",
    "* LSTM introduces a more complex memory unit called a cell state.\n",
    "* LSTM regulates the flow of information using three key gates:\n",
    "    * Forget Gate - Decides what information to forget\n",
    "    * Input Gate - Decides what new information to store\n",
    "    * Output Gate - Decides what to output based on the cell state\n",
    "\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "At each time step t, LSTM computes:\n",
    "1. Forget Gate: <br> Decides what information from the previous cell state to forget $$f_t = \\sigma(W_f*[h_{t-1}, x_t] + b_f) $$\n",
    "2. Input Gate: <br> Decides what new information to store in the cell state: $$i_t = \\sigma(W_i*[h_{t-1},x_t] + b_i) $$ $$C_t = tanh(W_c*[h_{t-1},x_t] +b_c)$$\n",
    "3. Cell State Update: <br> Update the cell state: $$C_t = f_t*C_{t-1} + i_t*C_t$$\n",
    "4. Output Gate: <br> Decide what to output: $$o_t = \\sigma(W_o*[h_{t-1},x_t] + b_o$$ $$h_t = o_t*tanh(C_t)$$\n",
    "where: <br>\n",
    "* $f_t, i_t, o_t = forget, input, and output gate activations$\n",
    "* $\\sigma$ = sigmoid activation function (range: 0 to 1)\n",
    "* $C_t$ = candidate cell state update\n",
    "\n",
    "### **Why LSTM Works Better Than RNN**\n",
    "* Better at capturing long-term dependencies\n",
    "* Solves vanishing gradient problem using forget gates\n",
    "* Selectively remembers useful information\n",
    "\n",
    "\n",
    "### **When to Use What**\n",
    "* Use RNN - If the sequence is short or dependencies are simple\n",
    "* Use LSTM - If the sequence is long and requires learning long-term dependencies."
   ],
   "id": "abfb1a7c3f38358c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa5390e0c5d19679"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
